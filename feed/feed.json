{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "Nick Jantz",
  "language": "en",
  "home_page_url": "https://nickjantz.com/",
  "feed_url": "https://nickjantz.com/feed/feed.json",
  "description": "I am writing about anything I want",
  "author": {
    "name": "Nick Jantz",
    "url": "https://nickjantz.com/about-me/"
  },
  "items": [{
      "id": "https://nickjantz.com/posts/ukulele/terminology/",
      "url": "https://nickjantz.com/posts/ukulele/terminology/",
      "title": "Ukulele Chord Magic Terminology",
      "content_html": "<h2>Background</h2>\n",
      "date_published": "2024-01-27T00:00:00Z"
    },{
      "id": "https://nickjantz.com/posts/ukulele/magic-part-1/",
      "url": "https://nickjantz.com/posts/ukulele/magic-part-1/",
      "title": "Ukulele Chord Magic",
      "content_html": "<h2>Background</h2>\n",
      "date_published": "2024-01-27T00:00:00Z"
    },{
      "id": "https://nickjantz.com/posts/supabase-flask/",
      "url": "https://nickjantz.com/posts/supabase-flask/",
      "title": "Protecting a Flask API with Supabase",
      "content_html": "<h2>Background</h2>\n<p>I am working on a side project that involves user authorization and authentication. In the past I have used <a href=\"http://www.passportjs.org/\">PassportJS</a>. While this has been great in the past there's some scenarios where it isn't the greatest. In this case I am using the OpenAI API which I've implemented in Python. Using Passport would require NodeJS and I don't want to cross the two languages as part of my backend.</p>\n<p>For this case I've decided to use <a href=\"https://supabase.com/\">Supabase</a>. It's an alternative to Firebase, and while it has grown to have a lot of features, what I'm interested in is using it as an auth provider. The Python library for it is only community supported, but it does have enough features to use as a basic auth wrapper. Mixed with some features of flask it will work just fine for my needs.</p>\n",
      "date_published": "2023-03-02T00:00:00Z"
    },{
      "id": "https://nickjantz.com/posts/kona-data-part-1/",
      "url": "https://nickjantz.com/posts/kona-data-part-1/",
      "title": "Kona Run Data Analysis Part 1",
      "content_html": "<h2>Background</h2>\n<p>Entalpi, the company that works with Gustav Iden and Kristian Blummenfelt recently released some raw data from the run at Kona 2022.</p>\n<p>I am going to be taking a basic look at some of the data released in a series of Jupyter Notebooks as a way of further my skills.</p>\n<p>Data and more info can be found on their Github here:\n<a href=\"https://github.com/entalpi-no/kona-2022\">https://github.com/entalpi-no/kona-2022</a></p>\n",
      "date_published": "2023-03-02T00:00:00Z"
    },{
      "id": "https://nickjantz.com/posts/interesting-maker-blog-article/",
      "url": "https://nickjantz.com/posts/interesting-maker-blog-article/",
      "title": "Article about taking over a failed IOT company",
      "content_html": "<p>I recently came across this post about taking over a dead IOT company that really intrigued me.</p>\n<p>https://blog.kchung.co/taking-over-a-dead-iot-company/</p>\n<p>The first reason I got into Development was through wanting to learn how to code after picking up a raspberry pi and a simple LED matrix from Alibaba. I had recently stumbled across the CTA open API and wanted to make essentially the exact same thing that this company tried to do. I even remember seeing this product on Facebook and Instagram ads and thinking, should I just buy this product instead of doing it myself.</p>\n<p>I even at one point had a dream of making a similar product and selling it, almost identically to using a raspberry pi in the frame to control an led matrix.</p>\n<p>I mainly wanted to share because I think the author takes an honest look at what happened not just from a tech perspective but also from a business perspective. It can be tough work running a product company that has to provide ongoing support and an API to devices in the world long after they're sold. For every company like this that actually ended up getting started and selling physical products, there's definitely about 100 entrepreneurs that had an idea but didn't take it all the way to sales because they couldn't figure out how to continue providing the service expected long after the physical product is in the customers hands.</p>\n",
      "date_published": "2023-01-12T00:00:00Z"
    },{
      "id": "https://nickjantz.com/posts/learning-with-large-datasets/",
      "url": "https://nickjantz.com/posts/learning-with-large-datasets/",
      "title": "Learning with Large Datasets",
      "content_html": "<p>In my learning data science I like to work with publicly available datasets. There are quite a few places to get this, but one that I keep going back to are some of the open data sets from <a href=\"https://data.cityofchicago.org/\">Chicago</a> and <a href=\"https://opendata.cityofnewyork.us/\">New York</a>. They just have great documentation and easy to download sets of data. The problem is they're often on the larger size and can take up a lot of computing resources for something like a Jupyter notebook where you may be working a lot when first learning.</p>\n<p>Luckily, you can go ahead and make mini versions of these datasets and then use them for any exploratory purposes before running full scripts on them, if you so choose. Here I will show you my approach to doing that. I realize there's ways to do this possibly by doing something like <code>ECHO head dataset.json &gt; mini-dataset.json</code> but for purposes of learning I wanted to test my python skills. <em>Edit note: see bottom of page for a simple 1 liner in bash</em></p>\n<p>My goal was to take all my data files, stored in a single folder and make versions of them starting with mini-*** to make them easier to work with. Maybe only 100-200 rows long.</p>\n<p>Since I'm doing this on all my data files I need to first gather them all in a list to loop through them. Coming from the web dev/JS world I am absolutely in love with list comprehensions.</p>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os<br /><br />files <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token builtin\">file</span> <span class=\"token keyword\">for</span> <span class=\"token builtin\">file</span> <span class=\"token keyword\">in</span> os<span class=\"token punctuation\">.</span>listdir<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">if</span> <span class=\"token builtin\">file</span><span class=\"token punctuation\">.</span>endswith<span class=\"token punctuation\">(</span><span class=\"token string\">'.csv'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></code></pre>\n<p>From there I will read through each file</p>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">for</span> <span class=\"token builtin\">file</span> <span class=\"token keyword\">in</span> files<span class=\"token punctuation\">:</span><br />    <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></code></pre>\n<p>Read the file into a pandas data frame</p>\n<pre class=\"language-python\"><code class=\"language-python\">read_file <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"data/</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token builtin\">file</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span></code></pre>\n<p>Write the first 150 lines into my new file</p>\n<pre class=\"language-python\"><code class=\"language-python\">read_file<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">150</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>to_csv<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"mini-</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token builtin\">file</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span></code></pre>\n<p>Final file looks like</p>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os<br /><span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd<br /><br />files <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token builtin\">file</span> <span class=\"token keyword\">for</span> <span class=\"token builtin\">file</span> <span class=\"token keyword\">in</span> os<span class=\"token punctuation\">.</span>listdir<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">if</span> <span class=\"token builtin\">file</span><span class=\"token punctuation\">.</span>endswith<span class=\"token punctuation\">(</span><span class=\"token string\">'.csv'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><br /><br /><span class=\"token keyword\">for</span> <span class=\"token builtin\">file</span> <span class=\"token keyword\">in</span> files<span class=\"token punctuation\">:</span><br />  read_file <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"data/</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token builtin\">file</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span><br />  read_file<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token number\">150</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>to_csv<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"mini-</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token builtin\">file</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span></code></pre>\n<p>As mentioned above the 1 liner in bash to achieve the same:</p>\n<pre><code>for FILE in *.csv; do head -n 151 $FILE &gt; &quot;mini-$FILE&quot;; done\n</code></pre>\n",
      "date_published": "2022-12-30T00:00:00Z"
    }
  ]
}
